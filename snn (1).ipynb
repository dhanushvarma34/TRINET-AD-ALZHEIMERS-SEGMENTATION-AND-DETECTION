{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3ue6-CDNVfGk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe45749",
        "outputId": "de941bde-a532-40f1-edeb-19173a388eff"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "image_directory = '/content/'\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.jpg')]\n",
        "image_files.sort()\n",
        "\n",
        "loaded_images = []\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "    img = Image.open(image_path)\n",
        "    img_array = np.array(img)\n",
        "    loaded_images.append(img_array)\n",
        "\n",
        "print(f\"Loaded {len(loaded_images)} images.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92c72fef"
      },
      "source": [
        "## U-net segmentation\n",
        "\n",
        "### Subtask:\n",
        "Apply a U-Net model to the loaded images to obtain segmentation masks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7644adf5",
        "outputId": "535a63db-b737-4cbe-ad0b-503375978b72"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def unet_model(input_size=(256, 256, 3)):\n",
        "    inputs = tf.keras.layers.Input(input_size)\n",
        "\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c2)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c1])\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "target_size = (256, 256)\n",
        "\n",
        "preprocessed_images = []\n",
        "for img_array in loaded_images:\n",
        "\n",
        "    print(f\"Processing image with shape: {img_array.shape}\")\n",
        "    try:\n",
        "        img_resized = tf.image.resize(img_array, target_size)\n",
        "        img_normalized = img_resized / 255.0\n",
        "        if len(img_normalized.shape) == 2:\n",
        "            img_processed = tf.expand_dims(img_normalized, axis=-1)\n",
        "            img_processed = tf.image.grayscale_to_rgb(img_processed)\n",
        "        elif len(img_normalized.shape) == 3 and img_normalized.shape[-1] == 3:\n",
        "            img_processed = img_normalized\n",
        "        else:\n",
        "            print(f\"Warning: Skipping image with unexpected shape {img_processed.shape}\")\n",
        "            continue\n",
        "\n",
        "        img_processed = tf.expand_dims(img_processed, axis=0)\n",
        "        preprocessed_images.append(img_processed)\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image with shape {img_array.shape}: {e}\")\n",
        "\n",
        "\n",
        "model = unet_model(input_size=(target_size[0], target_size[1], 3))\n",
        "\n",
        "segmentation_masks = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    mask = model.predict(preprocessed_img)\n",
        "    mask = tf.squeeze(mask, axis=0)\n",
        "    segmentation_masks.append(mask)\n",
        "\n",
        "print(f\"Generated {len(segmentation_masks)} segmentation masks.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "Generated 16 segmentation masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff7f5b96",
        "outputId": "0eb8a9e7-ad9d-4b74-f8e0-9f5a4c95eaa5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Load images\n",
        "image_directory = '/content/'\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.jpg')]\n",
        "image_files.sort() # Ensure consistent order\n",
        "\n",
        "loaded_images = []\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        img_array = np.array(img)\n",
        "        loaded_images.append(img_array)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_file}: {e}\")\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(loaded_images)} images.\")\n",
        "\n",
        "\n",
        "# Define or load the U-Net model\n",
        "# For this example, we'll define a simple placeholder U-Net structure\n",
        "# In a real scenario, you would load a pre-trained model weights\n",
        "def unet_model(input_size=(256, 256, 3)):\n",
        "    inputs = tf.keras.layers.Input(input_size)\n",
        "\n",
        "    # Contraction path (example layers)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # ... (more layers for a full U-Net)\n",
        "\n",
        "    # Expansive path (example layers)\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c2) # Example upsampling\n",
        "    u7 = tf.keras.layers.concatenate([u7, c1]) # Example skip connection\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c7) # Output layer for binary segmentation\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "# Assuming a target size for the U-Net input\n",
        "target_size = (256, 256)\n",
        "\n",
        "preprocessed_images = []\n",
        "for img_array in loaded_images:\n",
        "    # Check shape before resizing\n",
        "    print(f\"Processing image with shape: {img_array.shape}\")\n",
        "    # Resize image\n",
        "    try:\n",
        "        img_resized = tf.image.resize(img_array, target_size)\n",
        "        # Normalize pixel values (assuming images are 0-255)\n",
        "        img_normalized = img_resized / 255.0\n",
        "        # Add channel dimension if grayscale (U-Net expects 3 channels usually, or 1 for grayscale)\n",
        "        # Check the shape of the image to determine if it's grayscale or color\n",
        "        if len(img_normalized.shape) == 2: # Grayscale\n",
        "            img_processed = tf.expand_dims(img_normalized, axis=-1) # Add channel dimension\n",
        "            img_processed = tf.image.grayscale_to_rgb(img_processed) # Convert to 3 channels\n",
        "        elif len(img_normalized.shape) == 3 and img_normalized.shape[-1] == 3: # Color\n",
        "            img_processed = img_normalized\n",
        "        else: # Handle unexpected shapes\n",
        "            print(f\"Warning: Skipping image with unexpected shape {img_processed.shape}\")\n",
        "            continue\n",
        "\n",
        "        # Add batch dimension for the model\n",
        "        img_processed = tf.expand_dims(img_processed, axis=0)\n",
        "        preprocessed_images.append(img_processed)\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image with shape {img_array.shape}: {e}\")\n",
        "\n",
        "\n",
        "# Instantiate the model (using the target size derived from preprocessing)\n",
        "# Adjust input_size if your images are grayscale and you adapt the model\n",
        "model = unet_model(input_size=(target_size[0], target_size[1], 3)) # Assuming 3 channels after processing\n",
        "\n",
        "segmentation_masks = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    # Predict segmentation mask\n",
        "    mask = model.predict(preprocessed_img)\n",
        "    # Remove batch dimension\n",
        "    mask = tf.squeeze(mask, axis=0)\n",
        "    segmentation_masks.append(mask)\n",
        "\n",
        "print(f\"Generated {len(segmentation_masks)} segmentation masks.\")\n",
        "\n",
        "\n",
        "# Define a simple convolutional layer as a placeholder for GAN feature extraction\n",
        "# In a real scenario, this would be part of a loaded GAN discriminator or generator\n",
        "feature_extractor = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "\n",
        "extracted_features = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    # Apply the feature extractor to the image.\n",
        "    # Ensure the image has a batch dimension, which was added during U-Net preprocessing.\n",
        "    features = feature_extractor(preprocessed_img)\n",
        "    # Remove the batch dimension before storing\n",
        "    features = tf.squeeze(features, axis=0)\n",
        "    extracted_features.append(features.numpy()) # Convert to numpy array for easier handling\n",
        "\n",
        "print(f\"Extracted features for {len(extracted_features)} images.\")\n",
        "\n",
        "\n",
        "# Define a simple SNN model placeholder using TensorFlow\n",
        "# This is a basic feedforward structure with a spiking neuron model\n",
        "# In a real SNN, you would use specific spiking neuron layers (e.g., LIF neurons)\n",
        "# and potentially recurrent connections. This placeholder uses standard layers\n",
        "# and simulates spiking by thresholding.\n",
        "class SimpleSNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleSNN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(num_classes, activation='sigmoid') # Using sigmoid for binary classification probability\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Assuming inputs are flattened spike trains\n",
        "        x = self.dense1(inputs)\n",
        "        # Simulate spiking activity by thresholding (simple example)\n",
        "        spike_activity_dense1 = tf.cast(x > 0.5, tf.float32) # Placeholder for spiking mechanism\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        spike_activity_dense2 = tf.cast(x > 0.5, tf.float32) # Placeholder for spiking mechanism\n",
        "\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        # In a real SNN, you would accumulate spikes over time or use rate coding\n",
        "        # For this simple placeholder, we'll just return the final output and\n",
        "        # the simulated spike activity patterns from the dense layers.\n",
        "        return output, [spike_activity_dense1, spike_activity_dense2]\n",
        "\n",
        "# Instantiate the SNN model\n",
        "snn_model = SimpleSNN(num_classes=2)\n",
        "\n",
        "subject_predictions = []\n",
        "spike_activity_patterns = []\n",
        "\n",
        "# Combine and encode\n",
        "spike_trains = []\n",
        "\n",
        "# Ensure both lists have the same length\n",
        "if len(segmentation_masks) != len(extracted_features):\n",
        "    print(\"Error: The number of segmentation masks and extracted features do not match.\")\n",
        "else:\n",
        "    for i in range(len(segmentation_masks)):\n",
        "        mask = segmentation_masks[i]\n",
        "        features = extracted_features[i]\n",
        "\n",
        "        # Ensure mask and features have compatible shapes for concatenation\n",
        "        # Assuming masks are (height, width, 1) and features are (height, width, channels)\n",
        "        # If shapes are different, resize the mask to match the feature map size\n",
        "        if mask.shape[:2] != features.shape[:2]:\n",
        "            # Resize mask to match feature dimensions\n",
        "            mask_resized = tf.image.resize(mask, features.shape[:2])\n",
        "            # Ensure mask_resized has a channel dimension if it was lost during resizing\n",
        "            if len(mask_resized.shape) == 2:\n",
        "                 mask_resized = tf.expand_dims(mask_resized, axis=-1)\n",
        "            mask = mask_resized\n",
        "\n",
        "        # Concatenate the mask and features along the channel dimension\n",
        "        combined_features = tf.concat([mask, features], axis=-1)\n",
        "\n",
        "        # Flatten the combined features for encoding\n",
        "        flattened_features = tf.reshape(combined_features, [-1])\n",
        "\n",
        "        # Simple rate coding: firing rate proportional to feature value\n",
        "        # This is a placeholder and can be replaced with more sophisticated encoding\n",
        "        # For simplicity, we'll threshold and create binary spikes\n",
        "        threshold = 0.1  # Example threshold - MODIFIED\n",
        "        spike_train = (flattened_features > threshold).numpy().astype(int) # Convert boolean to int (0 or 1)\n",
        "\n",
        "        spike_trains.append(spike_train)\n",
        "\n",
        "    print(f\"Generated {len(spike_trains)} spike trains.\")\n",
        "    # Display the shape of the first spike train as an example\n",
        "    if spike_trains:\n",
        "        print(f\"Shape of the first spike train: {spike_trains[0].shape}\")\n",
        "\n",
        "\n",
        "# Process each spike train\n",
        "for spike_train in spike_trains:\n",
        "    # Ensure the spike train has a batch dimension\n",
        "    spike_train_batched = tf.expand_dims(tf.cast(spike_train, tf.float32), axis=0)\n",
        "\n",
        "    # Get prediction and spike activity\n",
        "    predictions, activity_patterns = snn_model(spike_train_batched)\n",
        "\n",
        "    # Get the class prediction (AD or CN)\n",
        "    # Assuming the output layer gives probabilities for [CN, AD]\n",
        "    predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "    # Map index to class label (assuming 0 for CN and 1 for AD)\n",
        "    class_label = \"AD\" if predicted_class_index == 1 else \"CN\"\n",
        "    subject_predictions.append(class_label)\n",
        "\n",
        "    # Store the spike activity patterns\n",
        "    # Convert TensorFlow tensors to numpy arrays for storage\n",
        "    activity_patterns_np = [act.numpy() for act in activity_patterns]\n",
        "    spike_activity_patterns.append(activity_patterns_np)\n",
        "\n",
        "print(f\"Processed {len(spike_trains)} spike trains.\")\n",
        "print(\"Captured spike activity patterns for each subject.\")\n",
        "\n",
        "# 1. Summarize the overall distribution of the predicted classes (AD/CN) across the subjects.\n",
        "print(\"Subject Predictions:\", subject_predictions)\n",
        "prediction_counts = {}\n",
        "for pred in subject_predictions:\n",
        "    prediction_counts[pred] = prediction_counts.get(pred, 0) + 1\n",
        "\n",
        "print(\"\\nClass Distribution Summary:\")\n",
        "for class_label, count in prediction_counts.items():\n",
        "    print(f\"{class_label}: {count}\")\n",
        "\n",
        "# 2. For each subject, analyze the stored spike activity patterns. Examine the temporal dynamics of spiking activity in the simulated dense layers.\n",
        "# Note: The current simple SNN model doesn't have explicit time steps. The \"temporal dynamics\"\n",
        "# here refers to the pattern of activation across neurons in the dense layers for a single input.\n",
        "# To truly show temporal dynamics, the SNN model would need to incorporate time steps.\n",
        "# We will visualize the activity patterns for the first few subjects as an example.\n",
        "\n",
        "print(\"\\nAnalyzing Spike Activity Patterns (first 3 subjects):\")\n",
        "for i in range(min(3, len(spike_activity_patterns))):\n",
        "    print(f\"  Subject {i+1}:\")\n",
        "    for layer_idx, activity in enumerate(spike_activity_patterns[i]):\n",
        "        print(f\"    Dense Layer {layer_idx + 1}:\")\n",
        "        # Visualize the activity pattern as a heatmap or just show descriptive stats\n",
        "        print(f\"      Shape: {activity.shape}\")\n",
        "        print(f\"      Min activity: {np.min(activity)}, Max activity: {np.max(activity)}, Mean activity: {np.mean(activity)}\")\n",
        "        # Optional: Plot a histogram of activity values for a more detailed view\n",
        "        # plt.figure(figsize=(8, 4))\n",
        "        # plt.hist(activity.flatten(), bins=50)\n",
        "        # plt.title(f'Subject {i+1} - Dense Layer {layer_idx + 1} Activity Distribution')\n",
        "        # plt.xlabel('Activity Value (Simulated Spike)')\n",
        "        # plt.ylabel('Frequency')\n",
        "        # plt.show()\n",
        "\n",
        "# 3. Calculate and analyze the average firing rate for neurons in each of the simulated dense layers across all subjects.\n",
        "# In this simple model, the 'firing rate' is the average activity (proportion of neurons with activity > 0.5)\n",
        "# across the neurons in a layer for a given input. We average this across all subjects.\n",
        "\n",
        "avg_firing_rates_by_layer = {}\n",
        "num_subjects = len(spike_activity_patterns)\n",
        "\n",
        "if num_subjects > 0:\n",
        "    # Assuming all subjects have the same number of layers and layer indices\n",
        "    num_layers = len(spike_activity_patterns[0])\n",
        "\n",
        "    for layer_idx in range(num_layers):\n",
        "        total_activity_sum = 0\n",
        "        total_neurons = 0\n",
        "        for subject_activity_list in spike_activity_patterns:\n",
        "            layer_activity = subject_activity_list[layer_idx]\n",
        "            total_activity_sum += np.sum(layer_activity)\n",
        "            total_neurons += layer_activity.size # Number of elements (neurons) in the layer's activity\n",
        "\n",
        "        if total_neurons > 0:\n",
        "            average_firing_rate = total_activity_sum / total_neurons\n",
        "            avg_firing_rates_by_layer[f\"Dense Layer {layer_idx + 1}\"] = average_firing_rate\n",
        "        else:\n",
        "             avg_firing_rates_by_layer[f\"Dense Layer {layer_idx + 1}\"] = 0.0 # Avoid division by zero\n",
        "\n",
        "print(\"\\nAverage Firing Rate Across Subjects by Layer:\")\n",
        "for layer, rate in avg_firing_rates_by_layer.items():\n",
        "    print(f\"{layer}: {rate:.4f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20 images.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "Generated 16 segmentation masks.\n",
            "Extracted features for 16 images.\n",
            "Generated 16 spike trains.\n",
            "Shape of the first spike train: (2162688,)\n",
            "Processed 16 spike trains.\n",
            "Captured spike activity patterns for each subject.\n",
            "Subject Predictions: ['CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN']\n",
            "\n",
            "Class Distribution Summary:\n",
            "CN: 16\n",
            "\n",
            "Analyzing Spike Activity Patterns (first 3 subjects):\n",
            "  Subject 1:\n",
            "    Dense Layer 1:\n",
            "      Shape: (1, 64)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.203125\n",
            "    Dense Layer 2:\n",
            "      Shape: (1, 32)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.09375\n",
            "  Subject 2:\n",
            "    Dense Layer 1:\n",
            "      Shape: (1, 64)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.203125\n",
            "    Dense Layer 2:\n",
            "      Shape: (1, 32)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.0625\n",
            "  Subject 3:\n",
            "    Dense Layer 1:\n",
            "      Shape: (1, 64)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.15625\n",
            "    Dense Layer 2:\n",
            "      Shape: (1, 32)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.125\n",
            "\n",
            "Average Firing Rate Across Subjects by Layer:\n",
            "Dense Layer 1: 0.1816\n",
            "Dense Layer 2: 0.0996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e094a6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `tensorflow` library is not installed. Install the library to resolve this dependency issue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwsG25q1XRNY",
        "outputId": "e8107fbc-f98d-483e-be17-8eca9cc2e7d5"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bbaed7e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that tensorflow is installed, retry the previous code block to load the U-Net model, preprocess the images, and generate segmentation masks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dc3be0a",
        "outputId": "90e10025-9efc-4c53-a1d8-d4ac231e9cc1"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define or load the U-Net model\n",
        "# For this example, we'll define a simple placeholder U-Net structure\n",
        "# In a real scenario, you would load a pre-trained model weights\n",
        "def unet_model(input_size=(256, 256, 3)):\n",
        "    inputs = tf.keras.layers.Input(input_size)\n",
        "\n",
        "    # Contraction path (example layers)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # ... (more layers for a full U-Net)\n",
        "\n",
        "    # Expansive path (example layers)\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c2) # Example upsampling\n",
        "    u7 = tf.keras.layers.concatenate([u7, c1]) # Example skip connection\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c7) # Output layer for binary segmentation\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "# Assuming a target size for the U-Net input\n",
        "target_size = (256, 256)\n",
        "\n",
        "preprocessed_images = []\n",
        "for img_array in loaded_images:\n",
        "    # Resize image\n",
        "    img_resized = tf.image.resize(img_array, target_size)\n",
        "    # Normalize pixel values (assuming images are 0-255)\n",
        "    img_normalized = img_resized / 255.0\n",
        "    # Add channel dimension if grayscale (U-Net expects 3 channels usually, or 1 for grayscale)\n",
        "    # Check the shape of the first image in loaded_images to determine if it's grayscale or color\n",
        "    if len(img_normalized.shape) == 2: # Grayscale\n",
        "        img_processed = tf.expand_dims(img_normalized, axis=-1) # Add channel dimension\n",
        "        img_processed = tf.image.grayscale_to_rgb(img_processed) # Convert to 3 channels\n",
        "    elif len(img_normalized.shape) == 3 and img_normalized.shape[-1] == 3: # Color\n",
        "        img_processed = img_normalized\n",
        "    else: # Handle unexpected shapes\n",
        "        print(f\"Warning: Skipping image with unexpected shape {img_normalized.shape}\")\n",
        "        continue\n",
        "\n",
        "    # Add batch dimension for the model\n",
        "    img_processed = tf.expand_dims(img_processed, axis=0)\n",
        "    preprocessed_images.append(img_processed)\n",
        "\n",
        "# Instantiate the model (using the target size derived from preprocessing)\n",
        "# Adjust input_size if your images are grayscale and you adapt the model\n",
        "model = unet_model(input_size=(target_size[0], target_size[1], 3)) # Assuming 3 channels after processing\n",
        "\n",
        "segmentation_masks = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    # Predict segmentation mask\n",
        "    mask = model.predict(preprocessed_img)\n",
        "    # Remove batch dimension\n",
        "    mask = tf.squeeze(mask, axis=0)\n",
        "    segmentation_masks.append(mask)\n",
        "\n",
        "print(f\"Generated {len(segmentation_masks)} segmentation masks.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Generated 11 segmentation masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b09cf99"
      },
      "source": [
        "## Gan feature extraction\n",
        "\n",
        "### Subtask:\n",
        "Apply a GAN model to the images to extract lesion-aware features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37cf5ac6"
      },
      "source": [
        "**Reasoning**:\n",
        "Define or load a GAN model architecture for feature extraction and iterate through the preprocessed images to extract features using the GAN model. Since a full GAN training or loading is not feasible without pre-trained weights or a specific model type, a placeholder approach will be used, simulating feature extraction by applying a convolutional layer to the preprocessed images as an example of a feature extraction step within a GAN-like pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad5737c3",
        "outputId": "b20a22c9-4225-4fda-fceb-fc8860404f22"
      },
      "source": [
        "feature_extractor = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "\n",
        "extracted_features = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    features = feature_extractor(preprocessed_img)\n",
        "    features = tf.squeeze(features, axis=0)\n",
        "    extracted_features.append(features.numpy())\n",
        "\n",
        "print(f\"Extracted features for {len(extracted_features)} images.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features for 16 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ace873f"
      },
      "source": [
        "## Combine and encode\n",
        "\n",
        "### Subtask:\n",
        "Combine the U-Net segmentation masks and GAN features and encode them into spike trains suitable for SNN input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb2fbe74"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the segmentation masks and extracted features and encode them into spike trains suitable for SNN input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef62b09e",
        "outputId": "4e286cf2-7312-4af7-e8ef-f0eae0f5c9af"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "spike_trains = []\n",
        "\n",
        "if len(segmentation_masks) != len(extracted_features):\n",
        "    print(\"Error: The number of segmentation masks and extracted features do not match.\")\n",
        "else:\n",
        "    for i in range(len(segmentation_masks)):\n",
        "        mask = segmentation_masks[i]\n",
        "        features = extracted_features[i]\n",
        "\n",
        "        if mask.shape[:2] != features.shape[:2]:\n",
        "            mask_resized = tf.image.resize(mask, features.shape[:2])\n",
        "            if len(mask_resized.shape) == 2:\n",
        "                 mask_resized = tf.expand_dims(mask_resized, axis=-1)\n",
        "            mask = mask_resized\n",
        "\n",
        "        combined_features = tf.concat([mask, features], axis=-1)\n",
        "\n",
        "        flattened_features = tf.reshape(combined_features, [-1])\n",
        "\n",
        "\n",
        "        threshold = 0.1\n",
        "        spike_train = (flattened_features > threshold).numpy().astype(int)\n",
        "\n",
        "        spike_trains.append(spike_train)\n",
        "\n",
        "    print(f\"Generated {len(spike_trains)} spike trains.\")\n",
        "    if spike_trains:\n",
        "        print(f\"Shape of the first spike train: {spike_trains[0].shape}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 16 spike trains.\n",
            "Shape of the first spike train: (2162688,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d031aea"
      },
      "source": [
        "## Snn processing\n",
        "\n",
        "### Subtask:\n",
        "Define or load the SNN model and process the spike trains to obtain subject-level class predictions (AD/CN) and spike activity patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e32b0db"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a simple SNN model using TensorFlow and process the spike trains to obtain subject-level class predictions and spike activity patterns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ea9d490",
        "outputId": "0bd4fcd2-48cc-4576-cd71-3a588304b07a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class SimpleSNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleSNN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        spike_activity_dense1 = tf.cast(x > 0.5, tf.float32)\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        spike_activity_dense2 = tf.cast(x > 0.5, tf.float32)\n",
        "\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "\n",
        "        return output, [spike_activity_dense1, spike_activity_dense2]\n",
        "\n",
        "snn_model = SimpleSNN(num_classes=2)\n",
        "\n",
        "subject_predictions = []\n",
        "spike_activity_patterns = []\n",
        "\n",
        "for spike_train in spike_trains:\n",
        "    spike_train_batched = tf.expand_dims(tf.cast(spike_train, tf.float32), axis=0)\n",
        "\n",
        "    predictions, activity_patterns = snn_model(spike_train_batched)\n",
        "\n",
        "    predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "    class_label = \"AD\" if predicted_class_index == 1 else \"CN\"\n",
        "    subject_predictions.append(class_label)\n",
        "\n",
        "    activity_patterns_np = [act.numpy() for act in activity_patterns]\n",
        "    spike_activity_patterns.append(activity_patterns_np)\n",
        "\n",
        "print(f\"Processed {len(spike_trains)} spike trains.\")\n",
        "print(\"Subject Predictions:\", subject_predictions)\n",
        "print(\"Captured spike activity patterns for each subject.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 16 spike trains.\n",
            "Subject Predictions: ['CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN']\n",
            "Captured spike activity patterns for each subject.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "350a8675",
        "outputId": "e2b2b1ab-01e5-4740-f27b-153f3e73fc08"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "image_directory = '/content/'\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.jpg')]\n",
        "image_files.sort()\n",
        "\n",
        "loaded_images = []\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        img_array = np.array(img)\n",
        "        loaded_images.append(img_array)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_file}: {e}\")\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(loaded_images)} images.\")\n",
        "\n",
        "\n",
        "def unet_model(input_size=(256, 256, 3)):\n",
        "    inputs = tf.keras.layers.Input(input_size)\n",
        "\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c2)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c1])\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "target_size = (256, 256)\n",
        "\n",
        "preprocessed_images = []\n",
        "for img_array in loaded_images:\n",
        "\n",
        "    print(f\"Processing image with shape: {img_array.shape}\")\n",
        "    try:\n",
        "        img_resized = tf.image.resize(img_array, target_size)\n",
        "        img_normalized = img_resized / 255.0\n",
        "        if len(img_normalized.shape) == 2:\n",
        "            img_processed = tf.expand_dims(img_normalized, axis=-1)\n",
        "            img_processed = tf.image.grayscale_to_rgb(img_processed)\n",
        "        elif len(img_normalized.shape) == 3 and img_normalized.shape[-1] == 3:\n",
        "            img_processed = img_normalized\n",
        "        else:\n",
        "            print(f\"Warning: Skipping image with unexpected shape {img_processed.shape}\")\n",
        "            continue\n",
        "\n",
        "        img_processed = tf.expand_dims(img_processed, axis=0)\n",
        "        preprocessed_images.append(img_processed)\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image with shape {img_array.shape}: {e}\")\n",
        "\n",
        "\n",
        "model = unet_model(input_size=(target_size[0], target_size[1], 3))\n",
        "\n",
        "segmentation_masks = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    mask = model.predict(preprocessed_img)\n",
        "    mask = tf.squeeze(mask, axis=0)\n",
        "    segmentation_masks.append(mask)\n",
        "\n",
        "print(f\"Generated {len(segmentation_masks)} segmentation masks.\")\n",
        "\n",
        "\n",
        "feature_extractor = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "\n",
        "extracted_features = []\n",
        "for preprocessed_img in preprocessed_images:\n",
        "    features = feature_extractor(preprocessed_img)\n",
        "    features = tf.squeeze(features, axis=0)\n",
        "    extracted_features.append(features.numpy())\n",
        "\n",
        "print(f\"Extracted features for {len(extracted_features)} images.\")\n",
        "\n",
        "\n",
        "class SimpleSNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleSNN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        spike_activity_dense1 = tf.cast(x > 0.5, tf.float32)\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        spike_activity_dense2 = tf.cast(x > 0.5, tf.float32)\n",
        "\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        return output, [spike_activity_dense1, spike_activity_dense2]\n",
        "\n",
        "snn_model = SimpleSNN(num_classes=2)\n",
        "\n",
        "subject_predictions = []\n",
        "spike_activity_patterns = []\n",
        "\n",
        "spike_trains = []\n",
        "\n",
        "if len(segmentation_masks) != len(extracted_features):\n",
        "    print(\"Error: The number of segmentation masks and extracted features do not match.\")\n",
        "else:\n",
        "    for i in range(len(segmentation_masks)):\n",
        "        mask = segmentation_masks[i]\n",
        "        features = extracted_features[i]\n",
        "\n",
        "        if mask.shape[:2] != features.shape[:2]:\n",
        "            mask_resized = tf.image.resize(mask, features.shape[:2])\n",
        "            if len(mask_resized.shape) == 2:\n",
        "                 mask_resized = tf.expand_dims(mask_resized, axis=-1)\n",
        "            mask = mask_resized\n",
        "\n",
        "        combined_features = tf.concat([mask, features], axis=-1)\n",
        "\n",
        "        flattened_features = tf.reshape(combined_features, [-1])\n",
        "\n",
        "        threshold = 0.1\n",
        "        spike_train = (flattened_features > threshold).numpy().astype(int)\n",
        "\n",
        "        spike_trains.append(spike_train)\n",
        "\n",
        "    print(f\"Generated {len(spike_trains)} spike trains.\")\n",
        "    if spike_trains:\n",
        "        print(f\"Shape of the first spike train: {spike_trains[0].shape}\")\n",
        "\n",
        "\n",
        "for spike_train in spike_trains:\n",
        "    spike_train_batched = tf.expand_dims(tf.cast(spike_train, tf.float32), axis=0)\n",
        "\n",
        "    predictions, activity_patterns = snn_model(spike_train_batched)\n",
        "\n",
        "    predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "    class_label = \"AD\" if predicted_class_index == 1 else \"CN\"\n",
        "    subject_predictions.append(class_label)\n",
        "\n",
        "    activity_patterns_np = [act.numpy() for act in activity_patterns]\n",
        "    spike_activity_patterns.append(activity_patterns_np)\n",
        "\n",
        "print(f\"Processed {len(spike_trains)} spike trains.\")\n",
        "print(\"Captured spike activity patterns for each subject.\")\n",
        "\n",
        "print(\"Subject Predictions:\", subject_predictions)\n",
        "prediction_counts = {}\n",
        "for pred in subject_predictions:\n",
        "    prediction_counts[pred] = prediction_counts.get(pred, 0) + 1\n",
        "\n",
        "print(\"\\nClass Distribution Summary:\")\n",
        "for class_label, count in prediction_counts.items():\n",
        "    print(f\"{class_label}: {count}\")\n",
        "\n",
        "print(\"\\nAnalyzing Spike Activity Patterns (first 3 subjects):\")\n",
        "for i in range(min(3, len(spike_activity_patterns))):\n",
        "    print(f\"  Subject {i+1}:\")\n",
        "    for layer_idx, activity in enumerate(spike_activity_patterns[i]):\n",
        "        print(f\"    Dense Layer {layer_idx + 1}:\")\n",
        "        print(f\"      Shape: {activity.shape}\")\n",
        "        print(f\"      Min activity: {np.min(activity)}, Max activity: {np.max(activity)}, Mean activity: {np.mean(activity)}\")\n",
        "\n",
        "avg_firing_rates_by_layer = {}\n",
        "num_subjects = len(spike_activity_patterns)\n",
        "\n",
        "if num_subjects > 0:\n",
        "    num_layers = len(spike_activity_patterns[0])\n",
        "\n",
        "    for layer_idx in range(num_layers):\n",
        "        total_activity_sum = 0\n",
        "        total_neurons = 0\n",
        "        for subject_activity_list in spike_activity_patterns:\n",
        "            layer_activity = subject_activity_list[layer_idx]\n",
        "            total_activity_sum += np.sum(layer_activity)\n",
        "            total_neurons += layer_activity.size\n",
        "\n",
        "        if total_neurons > 0:\n",
        "            average_firing_rate = total_activity_sum / total_neurons\n",
        "            avg_firing_rates_by_layer[f\"Dense Layer {layer_idx + 1}\"] = average_firing_rate\n",
        "        else:\n",
        "             avg_firing_rates_by_layer[f\"Dense Layer {layer_idx + 1}\"] = 0.0\n",
        "\n",
        "print(\"\\nAverage Firing Rate Across Subjects by Layer:\")\n",
        "for layer, rate in avg_firing_rates_by_layer.items():\n",
        "    print(f\"{layer}: {rate:.4f}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20 images.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (128, 128)\n",
            "Error preprocessing image with shape (128, 128): 'images' must have either 3 or 4 dimensions.\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "Processing image with shape: (248, 496, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "Generated 16 segmentation masks.\n",
            "Extracted features for 16 images.\n",
            "Generated 16 spike trains.\n",
            "Shape of the first spike train: (2162688,)\n",
            "Processed 16 spike trains.\n",
            "Captured spike activity patterns for each subject.\n",
            "Subject Predictions: ['CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'CN', 'AD', 'AD', 'AD', 'AD', 'AD', 'AD', 'CN', 'CN', 'CN']\n",
            "\n",
            "Class Distribution Summary:\n",
            "CN: 10\n",
            "AD: 6\n",
            "\n",
            "Analyzing Spike Activity Patterns (first 3 subjects):\n",
            "  Subject 1:\n",
            "    Dense Layer 1:\n",
            "      Shape: (1, 64)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.171875\n",
            "    Dense Layer 2:\n",
            "      Shape: (1, 32)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.09375\n",
            "  Subject 2:\n",
            "    Dense Layer 1:\n",
            "      Shape: (1, 64)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.203125\n",
            "    Dense Layer 2:\n",
            "      Shape: (1, 32)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.09375\n",
            "  Subject 3:\n",
            "    Dense Layer 1:\n",
            "      Shape: (1, 64)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.203125\n",
            "    Dense Layer 2:\n",
            "      Shape: (1, 32)\n",
            "      Min activity: 0.0, Max activity: 1.0, Mean activity: 0.09375\n",
            "\n",
            "Average Firing Rate Across Subjects by Layer:\n",
            "Dense Layer 1: 0.1611\n",
            "Dense Layer 2: 0.0840\n"
          ]
        }
      ]
    }
  ]
}